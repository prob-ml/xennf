{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch and Pyro imports\n",
    "import torch\n",
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceMeanField_ELBO\n",
    "from pyro.optim import Adam, PyroOptim\n",
    "from pyro.infer.mcmc.mcmc_kernel import MCMCKernel\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.transforms as T\n",
    "from pyro.distributions.transforms import Spline, ComposeTransform\n",
    "from pyro.distributions import TransformedDistribution\n",
    "\n",
    "# Utility imports\n",
    "import GPUtil\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom module imports\n",
    "from xenium_cluster import XeniumCluster\n",
    "from data import prepare_DLPFC_data, prepare_synthetic_data, prepare_Xenium_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"YAY! GPU available :3\")\n",
    "    \n",
    "    # Get all available GPUs sorted by memory usage (lowest first)\n",
    "    available_gpus = GPUtil.getAvailable(order='memory', limit=1)\n",
    "    \n",
    "    if available_gpus:\n",
    "        selected_gpu = available_gpus[0]\n",
    "        \n",
    "        # Set the GPU with the lowest memory usage\n",
    "        torch.cuda.set_device(selected_gpu)\n",
    "        torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        print(f\"Using GPU: {selected_gpu} with the lowest memory usage.\")\n",
    "    else:\n",
    "        print(\"No GPUs available with low memory usage.\")\n",
    "else:\n",
    "    print(\"No GPU available :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potts Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Potts prior is a prior distribution of cluster assignments given neighboring states. The general form of the Potts model is $$\\pi(z_i) \\propto \\exp \\left( \\sum_i H(z_i) + \\sum_{(i,j) \\in G} J(z_i, z_j)\\right)$$. H is the local potention representing some external influence of bias on the spot i. J is the interaction energy between neighboring sites. This represents the neighboring signals of spots, which is how we enforce the spatial clustering. The BayesSpace formulation feels like a more natural representation of this concept. $$\\pi(z_i) \\propto \\exp \\left( \\frac{\\gamma}{\\langle ij \\rangle} * 2\\sum_{\\langle ij \\rangle} I(z_i = z_j)\\right)$$ $\\gamma$ is a smoothing parameter which can be tuned to increase the spatial contiguity of spatial assignments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PottsPriorGibbs(MCMCKernel):\n",
    "    \n",
    "    def __init__(self, num_sites, num_clusters, neighbors, gamma):\n",
    "        \n",
    "        self.num_sites = num_sites\n",
    "        self.num_clusters = num_clusters\n",
    "        self.neighbors = neighbors\n",
    "        self.gamma = gamma\n",
    "        self.state = None\n",
    "\n",
    "    def setup(self, initial_state):\n",
    "        r\"\"\"\n",
    "        Optional method to set up any state required at the start of the\n",
    "        simulation run.\n",
    "\n",
    "        :param int warmup_steps: Number of warmup iterations.\n",
    "        :param \\*args: Algorithm specific positional arguments.\n",
    "        :param \\*\\*kwargs: Algorithm specific keyword arguments.\n",
    "        \"\"\"\n",
    "        self.state = initial_state\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"\n",
    "        Optional method to clean up any residual state on termination.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def logging(self):\n",
    "        \"\"\"\n",
    "        Relevant logging information to be printed at regular intervals\n",
    "        of the MCMC run. Returns `None` by default.\n",
    "\n",
    "        :return: String containing the diagnostic summary. e.g. acceptance rate\n",
    "        :rtype: string\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def diagnostics(self):\n",
    "        \"\"\"\n",
    "        Returns a dict of useful diagnostics after finishing sampling process.\n",
    "        \"\"\"\n",
    "        # NB: should be not None for multiprocessing works\n",
    "        return {}\n",
    "\n",
    "    def end_warmup(self):\n",
    "        \"\"\"\n",
    "        Optional method to tell kernel that warm-up phase has been finished.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def initial_params(self):\n",
    "        \"\"\"\n",
    "        Returns a dict of initial params (by default, from the prior) to initiate the MCMC run.\n",
    "\n",
    "        :return: dict of parameter values keyed by their name.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @initial_params.setter\n",
    "    def initial_params(self, params):\n",
    "        \"\"\"\n",
    "        Sets the parameters to initiate the MCMC run. Note that the parameters must\n",
    "        have unconstrained support.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, current_state):\n",
    "        \"\"\"\n",
    "        Samples parameters from the posterior distribution, when given existing parameters.\n",
    "\n",
    "        :param dict params: Current parameter values.\n",
    "        :param int time_step: Current time step.\n",
    "        :return: New parameters from the posterior distribution.\n",
    "        \"\"\"\n",
    "        \"\"\"Perform one Gibbs update over all sites.\"\"\"\n",
    "        new_state = current_state.clone()\n",
    "        \n",
    "        for i in range(self.num_sites):\n",
    "            # Compute conditional probabilities for site i\n",
    "            probs = torch.zeros(self.num_clusters)\n",
    "            for k in range(self.num_clusters):\n",
    "                # Compute the contribution of neighbors\n",
    "                probs[k] = torch.sum(\n",
    "                    torch.tensor([self.gamma if new_state[j] == k else 0.0 \n",
    "                                  for j in self.neighbors[i]])\n",
    "                )\n",
    "            \n",
    "            # Normalize to get valid probabilities\n",
    "            probs = torch.exp(probs - torch.max(probs))  # Avoid numerical issues\n",
    "            probs /= probs.sum()\n",
    "            \n",
    "            # Sample a new cluster label for site i\n",
    "            new_state[i] = torch.multinomial(probs, 1).item()\n",
    "        \n",
    "        self.state = new_state\n",
    "        return self.state\n",
    "\n",
    "    def __call__(self, params):\n",
    "        \"\"\"\n",
    "        Alias for MCMCKernel.sample() method.\n",
    "        \"\"\"\n",
    "        return self.sample(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the simulated dataset from the BayXenSmooth paper as an initial assessment of our idea. To make this happen we need to load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Properties\n",
    "batch_size = 128\n",
    "num_clusters = 7\n",
    "learn_global_variances = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_data, spatial_locations, original_adata, prior_weights = prepare_synthetic_data(num_clusters=num_clusters)\n",
    "prior_means = torch.zeros(num_clusters, gene_data.shape[1])\n",
    "prior_scales = torch.ones(num_clusters, gene_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(gene_data).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Given a Potts model for the prior and a GMM for the likelihood, how can we get a good posterior? \n",
    "\n",
    "In BayXenSmooth, we attempted to do this via an SVI framework where the posterior was approximated by another distribution with a desired paramteric form. This can be useful, but we want to model more complicated posteriors. This is where normalizing flows come in. By learning the posterior outcomes with complicated flows, we can plug in the flow as a variational distribution and train the variational inference procedure in a similar backprop way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior and Likelihood Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "NUM_PARTICLES = 25\n",
    "\n",
    "expected_total_param_dim = 2 # K x D\n",
    "\n",
    "def model(data):\n",
    "\n",
    "    with pyro.plate(\"clusters\", num_clusters):\n",
    "\n",
    "        # Define the means and variances of the Gaussian components\n",
    "        cluster_means = pyro.sample(\"cluster_means\", dist.Normal(prior_means, 10.0).to_event(1))\n",
    "        cluster_scales = pyro.sample(\"cluster_scales\", dist.LogNormal(prior_scales, 10.0).to_event(1))\n",
    "\n",
    "    # Define priors for the cluster assignment probabilities and Gaussian parameters\n",
    "    with pyro.plate(\"data\", len(data), subsample_size=batch_size) as ind:\n",
    "        batch_data = data[ind]\n",
    "        cluster_probs = pyro.sample(\"cluster_probs\", dist.Dirichlet(torch.ones(batch_size, num_clusters)))\n",
    "        # likelihood for batch\n",
    "        if cluster_means.dim() == expected_total_param_dim:\n",
    "            pyro.sample(f\"obs\", dist.MixtureOfDiagNormals(\n",
    "                    cluster_means.unsqueeze(0).expand(batch_size, -1, -1), \n",
    "                    cluster_scales.unsqueeze(0).expand(batch_size, -1, -1), \n",
    "                    cluster_probs\n",
    "                ), \n",
    "                obs=batch_data\n",
    "            )\n",
    "        # likelihood for batch WITH vectorization of particles\n",
    "        else:\n",
    "            pyro.sample(f\"obs\", dist.MixtureOfDiagNormals(\n",
    "                    cluster_means.unsqueeze(1).expand(-1, batch_size, -1, -1), \n",
    "                    cluster_scales.unsqueeze(1).expand(-1, batch_size, -1, -1), \n",
    "                    cluster_probs\n",
    "                ), \n",
    "                obs=batch_data\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizing_flow(K, batch_size):\n",
    "    # Base distribution: standard normal for the entire batch and feature dimensions\n",
    "    base_dist = dist.Normal(\n",
    "        torch.zeros(batch_size, K-1),\n",
    "        torch.ones(batch_size, K-1)\n",
    "    ).to_event(1)\n",
    "\n",
    "    # Define transformations\n",
    "    spline_transform = T.spline_coupling(K-1, count_bins=16)\n",
    "    # planar_transform = T.Planar(K-1)\n",
    "    stickbreak_transform = T.StickBreakingTransform()\n",
    "\n",
    "    # Apply transforms for the batch\n",
    "    flow_dist = dist.TransformedDistribution(base_dist, [spline_transform, stickbreak_transform])\n",
    "\n",
    "    # Register learnable parameters for optimization\n",
    "    pyro.module(\"spline_transform\", spline_transform)\n",
    "\n",
    "    return flow_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(data):\n",
    "    with pyro.plate(\"clusters\", num_clusters):\n",
    "        # Global variational parameters for cluster means and scales\n",
    "        cluster_means_q_mean = pyro.param(\"cluster_means_q_mean\", prior_means + torch.randn_like(prior_means) * 0.05)\n",
    "        cluster_scales_q_mean = pyro.param(\"cluster_scales_q_mean\", prior_scales + torch.randn_like(prior_scales) * 0.01, constraint=dist.constraints.positive)\n",
    "        if learn_global_variances:\n",
    "            cluster_means_q_scale = pyro.param(\"cluster_means_q_scale\", torch.ones_like(prior_means) * 1.0, constraint=dist.constraints.positive)\n",
    "            cluster_scales_q_scale = pyro.param(\"cluster_scales_q_scale\", torch.ones_like(prior_scales) * 0.25, constraint=dist.constraints.positive)\n",
    "            cluster_means = pyro.sample(\"cluster_means\", dist.Normal(cluster_means_q_mean, cluster_means_q_scale).to_event(1))\n",
    "            cluster_scales = pyro.sample(\"cluster_scales\", dist.LogNormal(cluster_scales_q_mean, cluster_scales_q_scale).to_event(1))\n",
    "        else:\n",
    "            cluster_means = pyro.sample(\"cluster_means\", dist.Delta(cluster_means_q_mean).to_event(1))\n",
    "            cluster_scales = pyro.sample(\"cluster_scales\", dist.Delta(cluster_scales_q_mean).to_event(1))\n",
    "\n",
    "    with pyro.plate(\"data\", len(data), subsample_size=batch_size) as ind:\n",
    "        batch_data = data[ind]\n",
    "\n",
    "        # Cluster probabilities\n",
    "        cluster_probs_flow_dist = normalizing_flow(num_clusters, batch_size)\n",
    "        cluster_probs = pyro.sample(\"cluster_probs\", cluster_probs_flow_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "NUM_EPOCHS = 1000\n",
    "NUM_BATCHES = int(math.ceil(data.shape[0] / batch_size))\n",
    "\n",
    "def per_param_callable(param_name):\n",
    "    if param_name == 'cluster_means_q_mean':\n",
    "        return {\"lr\": 0.01, \"betas\": (0.9, 0.999)}\n",
    "    elif param_name == 'cluster_scales_q_mean':\n",
    "        return {\"lr\": 0.01, \"betas\": (0.9, 0.999)}\n",
    "    elif \"logit\" in param_name:\n",
    "        return {\"lr\": 0.01, \"betas\": (0.9, 0.999)}\n",
    "    else:\n",
    "        return {\"lr\": 0.005, \"betas\": (0.9, 0.999)}\n",
    "\n",
    "scheduler = Adam(per_param_callable)\n",
    "\n",
    "# Setup the inference algorithm\n",
    "svi = SVI(model, guide, scheduler, loss=TraceMeanField_ELBO(num_particles=NUM_PARTICLES, vectorize_particles=True))\n",
    "\n",
    "# Create a DataLoader for the data\n",
    "# Convert data to CUDA tensors before creating the DataLoader\n",
    "data = data.to('cuda')\n",
    "\n",
    "# Clear the param store in case we're in a REPL\n",
    "pyro.clear_param_store()\n",
    "\n",
    "epoch_pbar = tqdm(range(NUM_EPOCHS))\n",
    "PATIENCE = 5\n",
    "patience_counter = 0\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_pbar.set_description(f\"Epoch {epoch}\")\n",
    "    running_loss = 0.0\n",
    "    for step in range(NUM_BATCHES):\n",
    "        loss = svi.step(data)\n",
    "        running_loss += loss / batch_size\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch} : loss = {round(running_loss, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriving Posterior Cluster Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster probabilities\n",
    "cluster_probs_flow_dist = normalizing_flow(num_clusters, batch_size)\n",
    "cluster_probs = pyro.sample(\"cluster_probs\", cluster_probs_flow_dist)\n",
    "cluster_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff-i93oUBlF-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
