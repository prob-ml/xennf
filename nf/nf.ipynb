{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch and Pyro imports\n",
    "import torch\n",
    "import zuko\n",
    "import numpy as np\n",
    "from torch import Size, Tensor\n",
    "import pyro\n",
    "import copy\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceMeanField_ELBO\n",
    "from pyro.optim import Adam, PyroOptim, ClippedAdam\n",
    "from pyro.infer.mcmc.mcmc_kernel import MCMCKernel\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.transforms as T\n",
    "from pyro.distributions.transforms import Spline, ComposeTransform\n",
    "from pyro.distributions import TransformedDistribution\n",
    "\n",
    "import torch_geometric as pyg\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, SGConv, SAGEConv, CuGraphSAGEConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric import EdgeIndex\n",
    "\n",
    "# Utility imports\n",
    "import GPUtil\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import ARI, NMI\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Custom module imports\n",
    "from xenium_cluster import XeniumCluster\n",
    "from data import prepare_DLPFC_data, prepare_synthetic_data, prepare_Xenium_data\n",
    "from zuko_flow import setup_zuko_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda setup\n",
    "if torch.cuda.is_available():\n",
    "    print(\"YAY! GPU available :3\")\n",
    "    \n",
    "    # Get all available GPUs sorted by memory usage (lowest first)\n",
    "    available_gpus = GPUtil.getAvailable(order='memory', limit=1)\n",
    "    \n",
    "    if available_gpus:\n",
    "        selected_gpu = available_gpus[0]\n",
    "        \n",
    "        # Set the GPU with the lowest memory usage\n",
    "        torch.cuda.set_device(selected_gpu)\n",
    "        torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        print(f\"Using GPU: {selected_gpu} with the lowest memory usage.\")\n",
    "    else:\n",
    "        print(\"No GPUs available with low memory usage.\")\n",
    "else:\n",
    "    print(\"No GPU available :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZukoToPyro(pyro.distributions.TorchDistribution):\n",
    "    r\"\"\"Wraps a Zuko distribution as a Pyro distribution.\n",
    "\n",
    "    If ``dist`` has an ``rsample_and_log_prob`` method, like Zuko's flows, it will be\n",
    "    used when sampling instead of ``rsample``. The returned log density will be cached\n",
    "    for later scoring.\n",
    "\n",
    "    :param dist: A distribution instance.\n",
    "    :type dist: torch.distributions.Distribution\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        flow = zuko.flows.MAF(features=5)\n",
    "\n",
    "        # flow() is a torch.distributions.Distribution\n",
    "\n",
    "        dist = flow()\n",
    "        x = dist.sample((2, 3))\n",
    "        log_p = dist.log_prob(x)\n",
    "\n",
    "        # ZukoToPyro(flow()) is a pyro.distributions.Distribution\n",
    "\n",
    "        dist = ZukoToPyro(flow())\n",
    "        x = dist((2, 3))\n",
    "        log_p = dist.log_prob(x)\n",
    "\n",
    "        with pyro.plate(\"data\", 42):\n",
    "            z = pyro.sample(\"z\", dist)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dist: torch.distributions.Distribution):\n",
    "        self.dist = dist\n",
    "        self.cache = {}\n",
    "\n",
    "    @property\n",
    "    def has_rsample(self) -> bool:\n",
    "        return self.dist.has_rsample\n",
    "\n",
    "    @property\n",
    "    def event_shape(self) -> Size:\n",
    "        return self.dist.event_shape\n",
    "\n",
    "    @property\n",
    "    def batch_shape(self) -> Size:\n",
    "        return self.dist.batch_shape\n",
    "\n",
    "    def __call__(self, shape: Size = ()) -> Tensor:\n",
    "        if hasattr(self.dist, \"rsample_and_log_prob\"):  # fast sampling + scoring\n",
    "            x, self.cache[x] = self.dist.rsample_and_log_prob(shape)\n",
    "        elif self.has_rsample:\n",
    "            x = self.dist.rsample(shape)\n",
    "        else:\n",
    "            x = self.dist.sample(shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def log_prob(self, x: Tensor) -> Tensor:\n",
    "        if x in self.cache:\n",
    "            return self.cache[x]\n",
    "        else:\n",
    "            return self.dist.log_prob(x)\n",
    "\n",
    "    def expand(self, *args, **kwargs):\n",
    "        return ZukoToPyro(self.dist.expand(*args, **kwargs))\n",
    "    \n",
    "    def rsample(self, sample_shape=torch.Size()):\n",
    "        return self.dist.rsample(sample_shape)  # Delegate to the underlying flow\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size()):\n",
    "        return self.dist.sample(sample_shape)  # Delegate to the underlying flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potts Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Potts prior is a prior distribution of cluster assignments given neighboring states. The general form of the Potts model is $$\\pi(z_i) \\propto \\exp \\left( \\sum_i H(z_i) + \\sum_{(i,j) \\in G} J(z_i, z_j)\\right)$$. H is the local potention representing some external influence of bias on the spot i. J is the interaction energy between neighboring sites. This represents the neighboring signals of spots, which is how we enforce the spatial clustering. The BayesSpace formulation feels like a more natural representation of this concept. $$\\pi(z_i) \\propto \\exp \\left( \\frac{\\gamma}{\\langle ij \\rangle} * 2\\sum_{\\langle ij \\rangle} I(z_i = z_j)\\right)$$ $\\gamma$ is a smoothing parameter which can be tuned to increase the spatial contiguity of spatial assignments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Potts2D(dist.Distribution):\n",
    "\n",
    "    def __init__(self, current_state, batch_idx, num_clusters=7, radius=1, gamma=1.5):\n",
    "        super().__init__()\n",
    "        self.current_state = current_state\n",
    "        self.batch_idx = batch_idx\n",
    "        self.num_clusters = num_clusters\n",
    "        self.radius = radius\n",
    "        self.gamma = gamma\n",
    "        self.num_rows, self.num_cols = current_state.shape\n",
    "\n",
    "    @property\n",
    "    def batch_shape(self):\n",
    "        # The shape of the grid\n",
    "        return torch.Size([len(self.batch_idx)])\n",
    "\n",
    "    @property\n",
    "    def event_shape(self):\n",
    "        # No event dimensions (this is over the whole grid)\n",
    "        return torch.Size([])\n",
    "\n",
    "    def expand(self, batch_shape, _instance=None):\n",
    "        # Create a new instance of Potts2D with the same parameters but a new batch shape\n",
    "        new_instance = Potts2D(\n",
    "            current_state=self.current_state,\n",
    "            batch_idx=self.batch_idx,\n",
    "            num_clusters=self.num_clusters,\n",
    "            radius=self.radius,\n",
    "            gamma=self.gamma,\n",
    "        )\n",
    "        return new_instance\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.sample()\n",
    "\n",
    "    def get_neighbors(self, i, j):\n",
    "        neighbors = []\n",
    "        for x in range(max(0, i - self.radius), min(self.current_state.shape[0], i + self.radius + 1)):\n",
    "            for y in range(max(0, j - self.radius), min(self.current_state.shape[1], j + self.radius + 1)):\n",
    "                if abs(x - i) + abs(y - j) <= self.radius:\n",
    "                    neighbors.append((x, y))\n",
    "        return neighbors\n",
    "\n",
    "    def sample(self):\n",
    "        num_rows, num_cols = self.current_state.shape\n",
    "\n",
    "        new_state = self.current_state.clone() \n",
    "        new_soft_state = torch.zeros(new_state.size(0), new_state.size(1), self.num_clusters)\n",
    "        \n",
    "        for i in range(num_rows):\n",
    "            for j in range(num_cols):\n",
    "            # Compute conditional probabilities for site i\n",
    "                probs = torch.zeros(self.num_clusters)\n",
    "                for k in range(1, self.num_clusters + 1):\n",
    "                    # Compute the contribution of neighbors\n",
    "                    probs[k-1] = torch.sum(\n",
    "                        torch.tensor(\n",
    "                            [2 * self.gamma if new_state[a][b] == k else 0.0 for (a, b) in self.get_neighbors(i, j)]\n",
    "                        )\n",
    "                    )\n",
    "                \n",
    "                # Normalize to get valid probabilities\n",
    "                probs = torch.exp(probs - torch.max(probs))  # Avoid numerical issues\n",
    "                probs /= probs.sum()\n",
    "                probs = torch.clamp(probs, min=0.0001)\n",
    "                new_soft_state[i][j] = probs\n",
    "                new_state[i][j] = torch.multinomial(probs, 1).item()\n",
    "\n",
    "        self.current_state = new_state\n",
    "        \n",
    "        MIN_PROB = 0.01\n",
    "        return torch.clamp(new_soft_state.reshape(-1, self.num_clusters)[self.batch_idx], min=MIN_PROB)\n",
    "\n",
    "    def log_prob(self, cluster_probs):\n",
    "\n",
    "        if cluster_probs.dim() == 2:\n",
    "\n",
    "            cluster_state_flattened = self.current_state.reshape(-1,1)[self.batch_idx]\n",
    "\n",
    "            # -1 is for indexing purposes. The 0 cluster is for empty cells.\n",
    "            # print(cluster_probs.shape)\n",
    "            # print(range(cluster_state_flattened.size(0)))\n",
    "            # print(cluster_state_flattened.flatten() - 1)\n",
    "            cluster_prob_tensor = cluster_probs[range(cluster_state_flattened.size(0)), cluster_state_flattened.flatten() - 1]\n",
    "\n",
    "            log_prob_tensor = torch.log(cluster_prob_tensor)\n",
    "\n",
    "        else:\n",
    "\n",
    "            cluster_state_flattened = self.current_state.reshape(-1,1)[self.batch_idx]\n",
    "\n",
    "            # -1 is for indexing purposes. The 0 cluster is for empty cells.\n",
    "            cluster_prob_tensor = cluster_probs[:, range(cluster_state_flattened.size(0)), cluster_state_flattened.flatten() - 1]\n",
    "\n",
    "            log_prob_tensor = torch.log(cluster_prob_tensor)\n",
    "\n",
    "        return log_prob_tensor  # Return the sum of all values in log_prob_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the simulated dataset from the BayXenSmooth paper as an initial assessment of our idea. To make this happen we need to load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Properties\n",
    "batch_size = 512\n",
    "data_dimension = 5\n",
    "num_clusters = 7\n",
    "learn_global_variances = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_data, spatial_locations, original_adata, TRUE_PRIOR_WEIGHTS = prepare_synthetic_data(num_clusters=num_clusters, data_dimension=data_dimension)\n",
    "prior_means = torch.zeros(num_clusters, gene_data.shape[1])\n",
    "prior_scales = torch.ones(num_clusters, gene_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(gene_data).float()\n",
    "num_obs, data_dim = data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Given a graph-based flow prior and a GMM for the likelihood, how can we get a good posterior? \n",
    "\n",
    "In BayXenSmooth, we attempted to do this via an SVI framework where the posterior was approximated by another distribution with a desired paramteric form. This can be useful, but we want to model more complicated posteriors. This is where normalizing flows come in. By learning the posterior outcomes with complicated flows, we can plug in the flow as a variational distribution and train the variational inference procedure in a similar backprop way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior and Likelihood Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Params to Set\n",
    "neighborhood_size = 1\n",
    "neighborhood_agg = \"mean\"\n",
    "num_pcs = 3\n",
    "\n",
    "def custom_cluster_initialization(original_adata, method, K=17):\n",
    "\n",
    "    original_adata.generate_neighborhood_graph(original_adata.xenium_spot_data, plot_pcas=False)\n",
    "\n",
    "    # This function initializes clusters based on the specified method\n",
    "    if method == \"K-Means\":\n",
    "        initial_clusters = original_adata.KMeans(original_adata.xenium_spot_data, save_plot=False, K=K, include_spatial=False)\n",
    "    elif method == \"Hierarchical\":\n",
    "        initial_clusters = original_adata.Hierarchical(original_adata.xenium_spot_data, save_plot=True, num_clusters=K)\n",
    "    elif method == \"Leiden\":\n",
    "        initial_clusters = original_adata.Leiden(original_adata.xenium_spot_data, resolutions=[0.47], save_plot=False, K=K)[0.47]\n",
    "    elif method == \"Louvain\":\n",
    "        initial_clusters = original_adata.Louvain(original_adata.xenium_spot_data, resolutions=[0.65], save_plot=False, K=K)[0.65]\n",
    "    elif method == \"mclust\":\n",
    "        original_adata.pca(original_adata.xenium_spot_data, num_pcs)\n",
    "        initial_clusters = original_adata.mclust(original_adata.xenium_spot_data, G=K, model_name = \"EEE\")\n",
    "    elif method == \"random\":\n",
    "        initial_clusters = np.random.randint(0, K, size=original_adata.xenium_spot_data.X.shape[0])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    return initial_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "# Clamping\n",
    "MIN_CONCENTRATION = 0.001\n",
    "\n",
    "num_posterior_samples = 10\n",
    "\n",
    "spatial_init_data = StandardScaler().fit_transform(gene_data)\n",
    "gene_data = StandardScaler().fit_transform(gene_data)\n",
    "empirical_prior_means = torch.zeros(num_clusters, spatial_init_data.shape[1])\n",
    "empirical_prior_scales = torch.ones(num_clusters, spatial_init_data.shape[1])\n",
    "\n",
    "rows = spatial_locations[\"row\"].astype(int)\n",
    "columns = spatial_locations[\"col\"].astype(int)\n",
    "\n",
    "num_rows = max(rows) + 1\n",
    "num_cols = max(columns) + 1\n",
    "\n",
    "initial_clusters = custom_cluster_initialization(original_adata, \"Louvain\", K=num_clusters)\n",
    "\n",
    "ari = ARI(initial_clusters, TRUE_PRIOR_WEIGHTS.argmax(axis=-1))\n",
    "nmi = NMI(initial_clusters, TRUE_PRIOR_WEIGHTS.argmax(axis=-1))\n",
    "cluster_metrics = {\n",
    "    \"ARI\": round(ari, 2),\n",
    "    \"NMI\": round(nmi, 2)\n",
    "}\n",
    "print(\"K-Means Metrics: \", cluster_metrics)\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    cluster_data = gene_data[initial_clusters == i]\n",
    "    if cluster_data.size > 0:  # Check if there are any elements in the cluster_data\n",
    "        empirical_prior_means[i] = torch.tensor(cluster_data.mean(axis=0))\n",
    "        empirical_prior_scales[i] = torch.tensor(cluster_data.std(axis=0))\n",
    "cluster_probs_prior = torch.zeros((initial_clusters.shape[0], num_clusters))\n",
    "cluster_probs_prior[torch.arange(initial_clusters.shape[0]), initial_clusters] = 1.\n",
    "\n",
    "locations_tensor = torch.tensor(spatial_locations.to_numpy())\n",
    "\n",
    "# Compute the number of elements in each dimension\n",
    "num_spots = cluster_probs_prior.shape[0]\n",
    "\n",
    "# Initialize an empty tensor for spatial concentration priors\n",
    "spatial_cluster_probs_prior = torch.zeros_like(cluster_probs_prior, dtype=torch.float64)\n",
    "\n",
    "spot_locations = KDTree(locations_tensor.cpu())  # Ensure this tensor is in host memory\n",
    "neighboring_spot_indexes = spot_locations.query_ball_point(locations_tensor.cpu(), r=neighborhood_size, p=1, workers=8)\n",
    "\n",
    "# Iterate over each spot\n",
    "for i in tqdm(range(num_spots)):\n",
    "\n",
    "    # Select priors in the neighborhood\n",
    "    priors_in_neighborhood = cluster_probs_prior[neighboring_spot_indexes[i]]\n",
    "\n",
    "    # Compute the sum or mean, or apply a custom weighting function\n",
    "    if neighborhood_agg == \"mean\":\n",
    "        neighborhood_priors = priors_in_neighborhood.mean(dim=0)\n",
    "    else:\n",
    "        locations = original_adata.xenium_spot_data.obs[[\"x_location\", \"y_location\", \"z_location\"]].values\n",
    "        neighboring_locations = locations[neighboring_spot_indexes[i]].astype(float)\n",
    "        distances = torch.tensor(np.linalg.norm(neighboring_locations - locations[i], axis=1))\n",
    "        def distance_weighting(x):\n",
    "            weight = 1/(1 + x/1)\n",
    "            # print(weight)\n",
    "            return weight / weight.sum()\n",
    "        neighborhood_priors = (priors_in_neighborhood * distance_weighting(distances).reshape(-1, 1)).sum(dim=0)\n",
    "    # Update the cluster probabilities\n",
    "    spatial_cluster_probs_prior[i] += neighborhood_priors\n",
    "\n",
    "spatial_cluster_probs_prior = spatial_cluster_probs_prior.clamp(MIN_CONCENTRATION)\n",
    "sample_for_assignment_options = [True, False]\n",
    "\n",
    "num_prior_samples = 10\n",
    "for sample_for_assignment in sample_for_assignment_options:\n",
    "\n",
    "    if sample_for_assignment:\n",
    "        cluster_assignments_prior_TRUE = pyro.sample(\"cluster_assignments\", dist.Categorical(spatial_cluster_probs_prior).expand_by([num_prior_samples])).detach().mode(dim=0).values\n",
    "        cluster_assignments_prior = cluster_assignments_prior_TRUE\n",
    "    else:\n",
    "        cluster_assignments_prior_FALSE = spatial_cluster_probs_prior.argmax(dim=1)\n",
    "        cluster_assignments_prior = cluster_assignments_prior_FALSE\n",
    "\n",
    "    # Load the data\n",
    "    data = torch.tensor(gene_data).float()\n",
    "\n",
    "    cluster_grid_PRIOR = torch.zeros((num_rows, num_cols), dtype=torch.long)\n",
    "\n",
    "    cluster_grid_PRIOR[rows, columns] = cluster_assignments_prior + 1\n",
    "\n",
    "    colors = plt.cm.get_cmap('viridis', num_clusters)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cluster_grid_PRIOR.cpu(), cmap=colors, interpolation='nearest', origin='lower')\n",
    "    plt.colorbar(ticks=range(1, num_clusters + 1), label='Cluster Values')\n",
    "    plt.title('Prior Cluster Assignment with BayXenSmooth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Editing the Hypernetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a graph of the spots.\n",
    "\n",
    "The idea is that the edges are connections between spots while the node attributes are gene expressions.\n",
    "\n",
    "We start by creating a regular normalizing flow and then updating the hypernetwork to be a GCN. Note that different flow types express their NNs as different attributes, so we need to have a helper function edit the hypernetwork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNFlowModel(nn.Module):\n",
    "    def __init__(self, original_graph, in_features, out_features, conv_type=\"GCN\"):\n",
    "        super().__init__()\n",
    "        self.x = original_graph.x\n",
    "        self.edge_index = original_graph.edge_index\n",
    "        self.set_layers(in_features, out_features, conv_type)\n",
    "\n",
    "    def set_layers(self, in_features, out_features, conv_type):\n",
    "        match conv_type:\n",
    "            case \"GCN\":\n",
    "                self.layers = nn.ModuleList([\n",
    "                    GCNModule(in_features, 512, conv_type),\n",
    "                    nn.ReLU(),\n",
    "                    GCNModule(512, 512, conv_type),\n",
    "                    nn.ReLU(),\n",
    "                    GCNModule(512, 512, conv_type),\n",
    "                    nn.ReLU(),\n",
    "                    GCNModule(512, out_features, conv_type)\n",
    "                ])\n",
    "            case \"SGCN\":\n",
    "                self.layers =  nn.ModuleList([\n",
    "                    GCNModule(in_features, out_features, conv_type),\n",
    "                ])\n",
    "            case \"SAGE\" | \"cuSAGE\":\n",
    "                self.layers = nn.ModuleList([\n",
    "                    GCNModule(in_features, 64, conv_type),\n",
    "                    nn.ReLU(),\n",
    "                    GCNModule(64, 64, conv_type),\n",
    "                    nn.ReLU(),\n",
    "                    GCNModule(64, 64, conv_type),\n",
    "                    nn.ReLU(),\n",
    "                    GCNModule(64, out_features, conv_type)\n",
    "                ])\n",
    "            case _:\n",
    "                raise NotImplementedError(f\"{conv_type} not supported yet.\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, GCNModule):\n",
    "                x = layer(x, self.edge_index)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GCNModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, conv=\"GCN\"):\n",
    "        super().__init__()\n",
    "        match conv:\n",
    "            case \"GCN\":\n",
    "                self.conv = GCNConv(in_channels, out_channels)\n",
    "            case \"SGCN\":\n",
    "                self.conv = SGConv(in_channels, out_channels, K=neighborhood_size)\n",
    "            case \"SAGE\":\n",
    "                self.conv = SAGEConv(in_channels, out_channels)\n",
    "            case \"cuSAGE\":\n",
    "                self.conv = CuGraphSAGEConv(in_channels, out_channels)\n",
    "            case _:\n",
    "                raise NotImplementedError(\"This convolutional layer does not have support.\")\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.conv(x, edge_index)\n",
    "\n",
    "def edit_flow_nn(flow, graph, graph_conv=\"GCN\"):\n",
    "    match type(flow):\n",
    "        case zuko.flows.continuous.CNF:\n",
    "            network = GCNFlowModel(\n",
    "                original_graph=graph,\n",
    "                in_features=flow.transform.ode[0].weight.shape[1],\n",
    "                out_features=num_clusters,\n",
    "                conv_type=graph_conv,\n",
    "            )\n",
    "            flow.transform.ode = network\n",
    "        case zuko.flows.autoregressive.MAF:\n",
    "            network = GCNFlowModel(\n",
    "                original_graph=graph,\n",
    "                in_features=num_clusters,\n",
    "                out_features=num_clusters*2,\n",
    "                conv_type=graph_conv,\n",
    "            )\n",
    "\n",
    "            flow.transform.transforms.insert(0, copy.deepcopy(flow.transform.transforms[0]))\n",
    "            flow.transform.transforms[0].hyper = network\n",
    "        case zuko.flow.spline.NSF:\n",
    "            pass\n",
    "        case _:\n",
    "            # Handle default case\n",
    "            pass\n",
    "    return flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = torch.tensor(spatial_locations.to_numpy()).float()\n",
    "edge_index = pyg.nn.knn_graph(positions, k=1+4*neighborhood_size, loop=True)\n",
    "edge_index = pyg.nn.knn_graph(positions, k=15, loop=True)\n",
    "GRAPH_CONV = \"GCN\"\n",
    "\n",
    "input_x = torch.tensor(original_adata.xenium_spot_data.X, dtype=torch.float32)\n",
    "graph = Data(x=data, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_probs_graph_flow_dist = setup_zuko_flow(\n",
    "    flow_type=\"MAF\",\n",
    "    flow_length=8,\n",
    "    num_clusters=num_clusters,\n",
    "    context_length=0,\n",
    "    hidden_layers=(512, 512, 512)\n",
    ")\n",
    "\n",
    "cluster_probs_graph_flow_dist = edit_flow_nn(cluster_probs_graph_flow_dist, graph, \"SAGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_probs_graph_flow_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "NUM_PARTICLES = 3\n",
    "\n",
    "expected_total_param_dim = 2 # K x D\n",
    "cluster_states = cluster_grid_PRIOR.clone()\n",
    "\n",
    "def model(data):\n",
    "\n",
    "    pyro.module(\"prior_flow\", cluster_probs_graph_flow_dist)\n",
    "    \n",
    "    with pyro.plate(\"clusters\", num_clusters):\n",
    "\n",
    "        # Define the EMPIRICAL means and variances of the Gaussian components\n",
    "        # cluster_means = pyro.sample(\"cluster_means\", dist.Normal(empirical_prior_means, 10.0).to_event(1))\n",
    "        # cluster_scales = pyro.sample(\"cluster_scales\", dist.LogNormal(empirical_prior_scales, 10.0).to_event(1))\n",
    "\n",
    "        # Define the BASIC means and variances of the Gaussian components\n",
    "        cluster_means = pyro.sample(\"cluster_means\", dist.Normal(prior_means, 10.0).to_event(1))\n",
    "        cluster_scales = pyro.sample(\"cluster_scales\", dist.LogNormal(prior_scales, 10.0).to_event(1))\n",
    "\n",
    "    cluster_logits = pyro.sample(\"cluster_logits\", ZukoToPyro(cluster_probs_graph_flow_dist()).expand([len(data)]).to_event(1))\n",
    "    # print(\"MODEL \", cluster_logits.shape, data.shape)\n",
    "\n",
    "    # Define priors for the cluster assignment probabilities and Gaussian parameters\n",
    "    with pyro.plate(\"data\", len(data), subsample_size=batch_size) as ind:\n",
    "        batch_data = data[ind]\n",
    "        batch_logits = cluster_logits[..., ind, :]\n",
    "        # print(\"BATCH DATA: \", batch_data.shape)\n",
    "        # print(\"BATCH LOGITS: \", batch_logits.shape)\n",
    "        # likelihood for batch\n",
    "        if cluster_means.dim() == expected_total_param_dim:\n",
    "            pyro.sample(f\"obs\", dist.MixtureOfDiagNormals(\n",
    "                    cluster_means.unsqueeze(0).expand(batch_size, -1, -1), \n",
    "                    cluster_scales.unsqueeze(0).expand(batch_size, -1, -1), \n",
    "                    batch_logits.squeeze(1)\n",
    "                ), \n",
    "                obs=batch_data\n",
    "            )\n",
    "        # likelihood for batch WITH vectorization of particles\n",
    "        else:\n",
    "            pyro.sample(f\"obs\", dist.MixtureOfDiagNormals(\n",
    "                    cluster_means.unsqueeze(1).expand(-1, batch_size, -1, -1), \n",
    "                    cluster_scales.unsqueeze(1).expand(-1, batch_size, -1, -1), \n",
    "                    batch_logits.squeeze(1)\n",
    "                ), \n",
    "                obs=batch_data\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_probs_flow_dist = setup_zuko_flow(\n",
    "    flow_type=\"MAF\",\n",
    "    flow_length=16,\n",
    "    num_clusters=num_clusters,\n",
    "    context_length=data_dim,\n",
    "    hidden_layers=(512, 512, 512)\n",
    ")\n",
    "\n",
    "def guide(data):\n",
    "    \n",
    "    pyro.module(\"posterior_flow\", cluster_probs_flow_dist)\n",
    "\n",
    "    with pyro.plate(\"clusters\", num_clusters):\n",
    "        # Global variational parameters for cluster means and scales (EMPIRICAL INIT)\n",
    "        # cluster_means_q_mean = pyro.param(\"cluster_means_q_mean\", empirical_prior_means + torch.randn_like(prior_means) * 0.05)\n",
    "        # cluster_scales_q_mean = pyro.param(\"cluster_scales_q_mean\", empirical_prior_scales + torch.randn_like(prior_scales) * 0.01, constraint=dist.constraints.positive)\n",
    "        \n",
    "        # Global variational parameters for cluster means and scales (BASIC INIT)\n",
    "        cluster_means_q_mean = pyro.param(\"cluster_means_q_mean\", prior_means + torch.randn_like(prior_means) * 0.05)\n",
    "        cluster_scales_q_mean = pyro.param(\"cluster_scales_q_mean\", prior_scales + torch.randn_like(prior_scales) * 0.01, constraint=dist.constraints.positive)\n",
    "        if learn_global_variances:\n",
    "            cluster_means_q_scale = pyro.param(\"cluster_means_q_scale\", torch.ones_like(prior_means) * 1.0, constraint=dist.constraints.positive)\n",
    "            cluster_scales_q_scale = pyro.param(\"cluster_scales_q_scale\", torch.ones_like(prior_scales) * 0.25, constraint=dist.constraints.positive)\n",
    "            cluster_means = pyro.sample(\"cluster_means\", dist.Normal(cluster_means_q_mean, cluster_means_q_scale).to_event(1))\n",
    "            cluster_scales = pyro.sample(\"cluster_scales\", dist.LogNormal(cluster_scales_q_mean, cluster_scales_q_scale).to_event(1))\n",
    "        else:\n",
    "            cluster_means = pyro.sample(\"cluster_means\", dist.Delta(cluster_means_q_mean).to_event(1))\n",
    "            cluster_scales = pyro.sample(\"cluster_scales\", dist.Delta(cluster_scales_q_mean).to_event(1))\n",
    "\n",
    "    cluster_logits = pyro.sample(\"cluster_logits\", ZukoToPyro(cluster_probs_flow_dist(data)).to_event(1))\n",
    "    # print(\"GUIDE \", cluster_logits.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66 : loss = -47.6992:   0%|          | 0/1000 [02:33<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "NUM_EPOCHS = 1000\n",
    "NUM_BATCHES = int(math.ceil(data.shape[0] / batch_size))\n",
    "LR = 1e-4\n",
    "\n",
    "def per_param_callable(param_name):\n",
    "    if param_name == 'cluster_means_q_mean':\n",
    "        return {\"lr\": 0.001, \"betas\": (0.9, 0.999)}\n",
    "    elif param_name == 'cluster_scales_q_mean':\n",
    "        return {\"lr\": 0.001, \"betas\": (0.9, 0.999)}\n",
    "    elif \"logit\" in param_name:\n",
    "        return {\"lr\": LR, \"betas\": (0.9, 0.999)}\n",
    "    else:\n",
    "        return {\"lr\": LR, \"betas\": (0.9, 0.999)}\n",
    "\n",
    "scheduler = Adam(per_param_callable)\n",
    "\n",
    "# Setup the inference algorithm\n",
    "svi = SVI(model, guide, scheduler, loss=TraceMeanField_ELBO(num_particles=NUM_PARTICLES, vectorize_particles=True))\n",
    "\n",
    "epoch_pbar = tqdm(range(NUM_EPOCHS))\n",
    "PATIENCE = 250\n",
    "current_min_loss = float('inf')\n",
    "patience_counter = 0\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    running_loss = 0.0\n",
    "    for step in range(NUM_BATCHES):\n",
    "        # with torch.amp.autocast(): THE OVERHEAD WASNT WORTH IT\n",
    "        loss = svi.step(data)\n",
    "        running_loss += loss / batch_size\n",
    "    epoch_pbar.set_description(f\"Epoch {epoch} : loss = {round(running_loss, 4)}\")\n",
    "    # print(f\"Epoch {epoch} : loss = {round(running_loss, 4)}\")\n",
    "    if running_loss > current_min_loss:\n",
    "        patience_counter += 1\n",
    "    else:\n",
    "        current_min_loss = running_loss\n",
    "        patience_counter = 0\n",
    "    if patience_counter >= PATIENCE:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model Results to Infinite Memory Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# save_path = os.path.join(\"/nfs/turbo/lsa-regier/scratch\", \"roko/nf_results\", \"test_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.get_param_store().save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.get_param_store().load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.get_param_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that should you decide to load a model, you will need to recreate the module with updated parameters as show in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_posterior_samples = 100\n",
    "cluster_probs_samples = []\n",
    "\n",
    "# pyro.module(\"posterior_flow\", cluster_probs_flow_dist, update_module_params=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_posterior_samples):\n",
    "        cluster_logits = pyro.sample(\"cluster_logits\", ZukoToPyro(cluster_probs_flow_dist(data)).to_event(1))\n",
    "\n",
    "        # Make the logits numerically stable\n",
    "        max_logit = torch.max(cluster_logits, dim=-1, keepdim=True).values\n",
    "        stable_logits = cluster_logits - max_logit\n",
    "        cluster_probs_sample = torch.nn.functional.softmax(stable_logits, dim=-1)\n",
    "        cluster_probs_samples.append(cluster_probs_sample)\n",
    "        del cluster_probs_sample  # Explicitly delete\n",
    "cluster_probs_avg = torch.stack(cluster_probs_samples).mean(dim=0)\n",
    "cluster_assignments_posterior = cluster_probs_avg.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# get the true and prior weights\n",
    "CLAMP = 0.000001\n",
    "spatial_cluster_probs_prior = spatial_cluster_probs_prior.clamp(CLAMP)\n",
    "TRUE_WEIGHTS = torch.tensor(TRUE_PRIOR_WEIGHTS).clamp(CLAMP)\n",
    "\n",
    "prior_cm = confusion_matrix(TRUE_WEIGHTS.argmax(dim=-1).cpu().numpy(), cluster_assignments_prior.cpu().numpy())\n",
    "posterior_cm = confusion_matrix(TRUE_WEIGHTS.argmax(dim=-1).cpu().numpy(), cluster_assignments_posterior.cpu().numpy())\n",
    "\n",
    "_, prior_permutation = linear_sum_assignment(cost_matrix = prior_cm, maximize=True)\n",
    "_, posterior_permutation = linear_sum_assignment(cost_matrix = posterior_cm, maximize=True)\n",
    "\n",
    "print(prior_permutation, posterior_permutation)\n",
    "# verify that the labelling doesn't magically switch during the learning process\n",
    "if not np.all(prior_permutation == posterior_permutation):\n",
    "    print(\"WARNING: Labels changed from prior to posterior.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate KL on the permuted inferred values.\n",
    "kl_prior_divergence = torch.sum(TRUE_WEIGHTS * torch.log(TRUE_WEIGHTS / spatial_cluster_probs_prior[:, prior_permutation]), dim=1)\n",
    "kl_post_divergence = torch.sum(TRUE_WEIGHTS * torch.log(TRUE_WEIGHTS / cluster_probs_avg[:, posterior_permutation]), dim=1)\n",
    "print(kl_prior_divergence.mean(), kl_post_divergence.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = -1\n",
    "TRUE_WEIGHTS[index], spatial_cluster_probs_prior[index], spatial_cluster_probs_prior[:, prior_permutation][index], cluster_probs_avg[index], cluster_probs_avg[:, posterior_permutation][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_KL_grid = torch.zeros(num_rows, num_cols, dtype=float)\n",
    "prior_KL_grid[rows, columns] = kl_prior_divergence\n",
    "cmap = plt.cm.get_cmap('coolwarm')\n",
    "plt.imshow(prior_KL_grid.cpu().numpy(), origin='lower', cmap=cmap)\n",
    "plt.colorbar(label='KL Divergence')\n",
    "plt.title(\"KL on Prior Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_KL_grid = torch.zeros(num_rows, num_cols, dtype=float)\n",
    "posterior_KL_grid[rows, columns] = kl_post_divergence\n",
    "cmap = plt.cm.get_cmap('coolwarm')\n",
    "plt.imshow(posterior_KL_grid.cpu().detach().numpy(), origin='lower', cmap=cmap)\n",
    "plt.colorbar(label='KL Divergence')\n",
    "plt.title(\"KL on Posterior Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = spatial_locations[\"row\"].astype(int)\n",
    "columns = spatial_locations[\"col\"].astype(int)\n",
    "\n",
    "num_rows = max(rows) + 1\n",
    "num_cols = max(columns) + 1\n",
    "\n",
    "cluster_grid = torch.zeros((num_rows, num_cols), dtype=torch.long)\n",
    "\n",
    "cluster_grid[rows, columns] = cluster_assignments_posterior + 1\n",
    "\n",
    "colors = plt.cm.get_cmap('viridis', num_clusters)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(cluster_grid.cpu(), cmap=colors, interpolation='nearest', origin='lower')\n",
    "plt.colorbar(ticks=range(1, num_clusters + 1), label='Cluster Values')\n",
    "plt.title('Posterior Cluster Assignment with XenNF')\n",
    "\n",
    "ari = ARI(cluster_assignments_posterior.cpu().numpy(), TRUE_PRIOR_WEIGHTS.argmax(axis=-1))\n",
    "nmi = NMI(cluster_assignments_posterior.cpu().numpy(), TRUE_PRIOR_WEIGHTS.argmax(axis=-1))\n",
    "cluster_metrics = {\n",
    "    \"ARI\": round(ari, 2),\n",
    "    \"NMI\": round(nmi, 2)\n",
    "}\n",
    "print(cluster_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.param(\"cluster_means_q_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_prior_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
